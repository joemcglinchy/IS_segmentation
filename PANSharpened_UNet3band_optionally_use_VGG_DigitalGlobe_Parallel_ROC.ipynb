{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "import cv2\n",
    "import rasterio\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "from dataset import load_image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from utils import variable\n",
    "from generate_masks import get_model\n",
    "from unet_models import unet11_MS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO ALTERED. THESE CELLS GENERATE SOME DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import the helper functions\n",
    "from test_unet_helpers import *\n",
    "\n",
    "import glob\n",
    "import os,sys\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "import shapely\n",
    "from rasterio.mask import mask\n",
    "from pyproj import Proj, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify scene id for DG data\n",
    "dg_scene_id = '1030010057062200'\n",
    "\n",
    "# specify image for 2.0 meter analysis\n",
    "gt_image_2 = '../rasters/union_impervious_raster_2_0_0_wgs84.tif' # desktop\n",
    "\n",
    "# specify images for 0.5 meter analysis\n",
    "gt_image_05 = '../rasters/union_impervious_raster_0_5_wgs84_SnapR.tif' # desktop\n",
    "\n",
    "# specify the shapefile\n",
    "shpfile = '../union/union.shp' # desktop\n",
    "rows = []\n",
    "with fiona.open(shpfile) as shp:\n",
    "    crs = shp.crs\n",
    "    shp_bounds = shp.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbdxtools import Interface, CatalogImage\n",
    "\n",
    "# connect to gbdx\n",
    "gbdx = Interface()\n",
    "\n",
    "# get the dask array for the 8 band MS image\n",
    "img_2m = CatalogImage(dg_scene_id, band_type='MS', bbox=shp_bounds, acomp=True)\n",
    "rows, cols = img_2m.shape[1:]\n",
    "\n",
    "# get the dask array for the Pansharpened, 4 band image\n",
    "image_05m = CatalogImage(dg_scene_id, bbox=shp_bounds, acomp=True, pansharpen=True)\n",
    "rowsP, colsP = image_05m.shape[1:]\n",
    "pan_factorR = np.floor(rowsP/rows)\n",
    "pan_factorC = np.floor(colsP/cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# generate the sample points\n",
    "# we have some funny stuff from ArcGIS converting the polygons to a raster... so... \n",
    "# generate the points within the bounds of the shapefile / DG image array\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "num = 10000\n",
    "side = 64\n",
    "coords = []\n",
    "coords_pan = []\n",
    "im_patches = []\n",
    "im_patches_pan = []\n",
    "\n",
    "with rasterio.open(gt_image_2, 'r') as src:\n",
    "    \n",
    "    for i in range(num):\n",
    "        ran_row, ran_col = random.randrange(0,rows), random.randrange(0,cols)\n",
    "        #print('generating sample {} of {}'.format(i+1, num))\n",
    "\n",
    "        try:\n",
    "            r_start = ran_row - side/2\n",
    "            r_end = ran_row + side/2\n",
    "            c_start = ran_col - side/2\n",
    "            c_end = ran_col + side/2\n",
    "            #im_data = img_2m[:, r_start:r_end, c_start:c_end]\n",
    "\n",
    "            pr_start = ran_row*pan_factorR - side/2\n",
    "            pr_end = ran_row*pan_factorR + side/2\n",
    "            pc_start = ran_col*pan_factorC - side/2\n",
    "            pc_end = ran_col*pan_factorC + side/2\n",
    "            #im_data_pan = image_05m[:, pr_start:pr_end, pc_start:pc_end]\n",
    "\n",
    "            # append regardless and check later.\n",
    "\n",
    "            #############################################################################################################\n",
    "            ### SHOULD CHECK THE INFORMATION THRESHOLDS BELOW, BUT HERE, TO RESELECT ROWS AND COLUMNS UNTIL IT IS MET ###\n",
    "            #############################################################################################################\n",
    "\n",
    "            s_pt = calcXYfromRC(img_2m.affine, (ran_row, ran_col))\n",
    "            pt_flag = checkWindow(src, s_pt, window_size=64)\n",
    "            ct = 0\n",
    "            while pt_flag:\n",
    "                ran_row, ran_col = random.randrange(0,rows), random.randrange(0,cols)\n",
    "                s_pt = calcXYfromRC(img_2m.affine, (ran_row, ran_col))\n",
    "                pt_flag = checkWindow(src, s_pt, window_size=64)\n",
    "                \n",
    "                ct+=1\n",
    "                # exit loop after 1000 tries\n",
    "                if ct > 1000:\n",
    "                    break\n",
    "            if ct > 1000:\n",
    "                print('1000 tries, moving on')\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            coords.append((ran_row, ran_col))\n",
    "            coords_pan.append(( int(ran_row*pan_factorR), int(ran_col*pan_factorC)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    coords_arr = np.array(coords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_MS = [calcXYfromRC(img_2m.affine, pair) for pair in coords]\n",
    "lonlat_PAN = [calcXYfromRC(image_05m.affine, pair) for pair in coords_pan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "       \n",
    "def assignRC2(rio_obj, samp_pt, window_size=64, inproj='epsg:4326', outproj='epsg:32613'):\n",
    "    # project the point to source crs\n",
    "    outProj = Proj(init=outproj)\n",
    "    inProj = Proj(init=inproj)\n",
    "    x1,y1 = samp_pt        \n",
    "    x2,y2 = pyproj.transform(inProj, outProj ,x1,y1)\n",
    "    x3,y3 = pyproj.transform(outProj, inProj, x2,y2)\n",
    "    \n",
    "    # get the row column\n",
    "    temp = rio_obj.index(x2,y2)\n",
    "    r,c = [int(c) for c in temp]\n",
    "    \n",
    "    return ((x3,y3), samp_pt, r,c)\n",
    "\n",
    "def assignRC3(rio_obj, samp_pt, window_size=64, inproj='epsg:4326', outproj='epsg:32613'):\n",
    "    # project the point to source crs\n",
    "    #outProj = Proj(init=outproj)\n",
    "    #inProj = Proj(init=inproj)\n",
    "    x1,y1 = samp_pt        \n",
    "    #x2,y2 = pyproj.transform(inProj, outProj ,x1,y1)\n",
    "    #x3,y3 = pyproj.transform(outProj, inProj, x2,y2)\n",
    "    \n",
    "    # get the row column\n",
    "    temp = rio_obj.index(x1,y1)\n",
    "    r,c = [int(c) for c in temp]\n",
    "    \n",
    "    return ((x1,y1), samp_pt, r,c)\n",
    "\n",
    "# use the lonlats_* lists to sample the ground truth image. return the histogram if the window is on the border,\n",
    "# which should be evident if the window extends across\n",
    "with rasterio.open(gt_image_2, 'r') as src:\n",
    "    #print(src.crs)\n",
    "\n",
    "    # cull the lon/lat points by checking if they include nodata pixels\n",
    "    gt_pts_MS = [assignRC3(src, samp_pt) for samp_pt in lonlat_MS]\n",
    "\n",
    "# do the same for the 0.5 meter ground truth raster\n",
    "with rasterio.open(gt_image_05, 'r') as src:\n",
    "\n",
    "    # cull the lon/lat points by checking if they include nodata pixels\n",
    "    gt_pts_PAN = [assignRC3(src, samp_pt) for samp_pt in lonlat_PAN]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transform for DG imagery. Top is 8 band MS, lower is 4 band Pansharped \n",
    "# img_transform = Compose([\n",
    "#     Normalize(mean=[1545.9403, 1460.1287, 1452.7002, 1466.6466, 1511.6293, 1902.1776, 2497.181, 2167.458], \n",
    "#               std=[512.423, 672.7396, 743.9062, 840.52625, 918.4014, 828.56976, 1196.6774, 1061.4962])\n",
    "# ])\n",
    "\n",
    "img_transform = Compose([\n",
    "    Normalize(mean=[1454.3645, 1346.8903, 1333.2897, 1319.6912], \n",
    "              std=[461.94232, 552.4788, 591.1144, 661.6691])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     print(image_05m[i,1000:5000,1000:5000].mean().compute(), image_05m[i,1000:5000,1000:5000].std().compute())\n",
    "# image_05m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the initial model with a subset of the sample points.. need to edit the source code to use the input data classes instead of data loaders.\n",
    "### to use the dataloader, may need to create some attributes on the class such as self.__len__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 9000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test_percentage = 0.2\n",
    "train_ind = int(num - num * val_test_percentage)\n",
    "val_ind = int((num - train_ind)*0.5)\n",
    "val_ind = train_ind + val_ind\n",
    "train_ind, val_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 100\n",
    "\n",
    "## establish the data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ground truth\n",
    "gt_transform = Compose([\n",
    "    ToTensor()\n",
    "    ])\n",
    "gt_dataset_train = gtDatasetSampler2(gt_image_05, coords_pan[:train_ind], transform=gt_transform, window_size=64) \n",
    "# gt_dataset_train = gtDatasetSampler2(gt_image_05, coords[:1000], transform=gt_transform) # debug \n",
    "gt_dataset_val = gtDatasetSampler2(gt_image_05, coords_pan[train_ind:val_ind], transform=gt_transform, window_size=64)\n",
    "gt_dataset_test = gtDatasetSampler2(gt_image_05, coords_pan[val_ind:], transform=gt_transform, window_size=64)\n",
    "\n",
    "gt_dl_train = DataLoader(gt_dataset_train, batch_size=bsize, shuffle=False)\n",
    "gt_dl_val = DataLoader(gt_dataset_val, batch_size=bsize, shuffle=False)\n",
    "\n",
    "\n",
    "#DG\n",
    "dg_dataset_train = DigitalGlobeSamplerTensor(image_05m, coords_pan[:train_ind], transform=img_transform, comb='bgrn', window_size=64) # will return RGB by default\n",
    "# dg_dataset_train = DigitalGlobeSamplerTensor(image_05m, coords[:1000], transform=img_transform) # debug\n",
    "dg_dataset_val = DigitalGlobeSamplerTensor(image_05m, coords_pan[train_ind:val_ind], transform=img_transform, comb='bgrn',window_size=64)\n",
    "dg_dataset_test = DigitalGlobeSamplerTensor(image_05m, coords_pan[val_ind:], transform=img_transform, comb='bgrn', window_size=64)\n",
    "\n",
    "dg_dl_train = DataLoader(dg_dataset_train, batch_size=bsize, shuffle=False)\n",
    "dg_dl_val = DataLoader(dg_dataset_val, batch_size=bsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "args = {}\n",
    "args['lr'] = 0.0001\n",
    "args['n_epochs'] = 4\n",
    "args['batch_size'] = bsize\n",
    "args['root'] = 'runs/debug'\n",
    "args['fold']=0\n",
    "args['jaccard_weight'] = 1\n",
    "\n",
    "# convert dict keys to objects and specify lr\n",
    "args = objectview(args)\n",
    "lr = args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def write_event(log, step: int, **data):\n",
    "    data['step'] = step\n",
    "    data['dt'] = datetime.now().isoformat()\n",
    "    log.write(json.dumps(data, sort_keys=True))\n",
    "    log.write('\\n')\n",
    "    log.flush()\n",
    "    \n",
    "save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "    }, str(model_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as pyt_utils\n",
    "from torch.optim import Adam\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pathlib import Path\n",
    "from validation import validation_binary\n",
    "from loss import LossBinary\n",
    "import json\n",
    "\n",
    "# need to change this to DICE loss!\n",
    "#loss = LossBinary(jaccard_weight=args.jaccard_weight)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "root = Path(args.root)\n",
    "root.mkdir(exist_ok=True, parents=True)\n",
    "root.joinpath('params.json').write_text(\n",
    "    json.dumps(vars(args), indent=True, sort_keys=True))\n",
    "\n",
    "valid = validation_binary\n",
    "\n",
    "num_classes=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY A LOCAL IMPLEMENTATION OF UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n",
    "                 batch_norm=False, up_mode='upconv'):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
    "                                                padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
    "                                            padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path)-1:\n",
    "                blocks.append(x)\n",
    "                x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i-1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
    "                                         stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "batchsize,  8\n",
      "training...\n",
      "on epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for epoch 1: 2571.1063220500946 seconds\n",
      "on epoch 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "# from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "## call outside training method, in loop\n",
    "\n",
    "# iterate over some batch sizes\n",
    "times = []\n",
    "# batch_sizes = [50, 100, 200,300,400,500,600,700,800,900,1000]\n",
    "bsize = 8 # smaller batch sizes as per https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n",
    "# model = unet11_MS(pretrained=False)\n",
    "model = UNet(in_channels=4, n_classes=1, padding=True)\n",
    "\n",
    "# set some parameters\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = LossBinary(jaccard_weight=args.jaccard_weight)\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# make model parallel and on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # get device for gpu or cpu\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = LossBinary(jaccard_weight=args.jaccard_weight)\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "print('batchsize, ', bsize)\n",
    "mean_loss = 0.0\n",
    "running_loss = 0.0\n",
    "print('training...')\n",
    "\n",
    "losses=[]\n",
    "losses_ep = []\n",
    "#tl = zip(dg_dl_train, gt_dl_train)\n",
    "step=0\n",
    "\n",
    "\n",
    "###### specify DataLoaders\n",
    "## GT\n",
    "gt_dl_train = DataLoader(gt_dataset_train, batch_size=bsize, shuffle=False, num_workers=4)\n",
    "gt_dl_val = DataLoader(gt_dataset_val, batch_size=bsize, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "## DG\n",
    "dg_dl_train = DataLoader(dg_dataset_train, batch_size=bsize, shuffle=False, num_workers=4)\n",
    "dg_dl_val = DataLoader(dg_dataset_val, batch_size=bsize, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# record time for one epoch\n",
    "t0 = time.time()\n",
    "for epoch in range(100):\n",
    "    \n",
    "    try:\n",
    "        te = time.time()\n",
    "\n",
    "        epoch_i = epoch+1\n",
    "        print('on epoch {}'.format(epoch_i))\n",
    "        tl = zip(dg_dl_train, gt_dl_train) # this can't be defined outside the loop. This suggests it is...cashing out??\n",
    "        for i, (inputs_, targets_) in enumerate(tl):\n",
    "\n",
    "            # get the inputs\n",
    "            #inputs, targets = variable(inputs_).cuda(), variable(targets_).cuda()\n",
    "            inputs, targets = variable(inputs_), variable(targets_)\n",
    "            inputs.to(device)\n",
    "            targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # zero parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize            \n",
    "            i_loss = criterion(outputs, targets)                \n",
    "            i_loss.backward()\n",
    "            optimizer.step()\n",
    "            step+=1\n",
    "\n",
    "\n",
    "            if i%10 == 0: # print/store every 10\n",
    "                losses.append(i_loss)\n",
    "                #running_loss += i_loss\n",
    "                #print('[%d, %5d] loss: %.3f' %(epoch, i + 1, i_loss))\n",
    "\n",
    "        # get some numbers for the ROC curve\n",
    "        if epoch_i%2 == 0:\n",
    "            pred_y = outputs.cpu().data.numpy().squeeze().flatten()\n",
    "            target_y = targets.cpu().data.numpy().flatten()\n",
    "            tu = (i_loss, roc_auc_score(target_y,pred_y ))\n",
    "            print ('epoch: {}, LOSS={}, ROC_AUC={} '.format(epoch_i, *tu))  \n",
    "\n",
    "        losses_ep.append(losses)\n",
    "        te1 = time.time()\n",
    "        print('time for epoch {}: {} seconds'.format(epoch_i, te1-te))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('something happened')\n",
    "        print(e)\n",
    "        break\n",
    "        \n",
    "t1 = time.time()\n",
    "print ('total time: ',t1 - t0)\n",
    "times.append(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y,pred_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('epoch: {}, LOSS= {}'.format(epoch_i, i_loss))\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([-0.1, 1.2])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Validation data\n",
    "## ground truth\n",
    "gt_dl_val = DataLoader(gt_dataset_val, batch_size=bsize, shuffle=False)\n",
    "\n",
    "\n",
    "## DG\n",
    "dg_dl_val = DataLoader(dg_dataset_val, batch_size=bsize, shuffle=False)\n",
    "\n",
    "\n",
    "# define a zipped object for iterating\n",
    "zl = zip(dg_dl_val, gt_dl_val)\n",
    "\n",
    "# some lists for plotting all batch ROC curves\n",
    "all_fpr = []\n",
    "all_tpr = []\n",
    "all_thresh = []\n",
    "all_roc_auc = []\n",
    "for i_val, (inputs_, targets_) in enumerate(zl):\n",
    "\n",
    "        # get the inputs\n",
    "        #inputs, targets = variable(inputs_).cuda(), variable(targets_).cuda()\n",
    "        inputs, targets = variable(inputs_), variable(targets_)\n",
    "        inputs.to(device)\n",
    "        targets.to(device)\n",
    "\n",
    "        # evaluate\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # record loss\n",
    "        v_loss = criterion(outputs, targets)\n",
    "        \n",
    "        # convert to numpy and get ROC params\n",
    "        pred_y_val = outputs.cpu().data.numpy().squeeze().flatten()\n",
    "        #pred_y_val[pred_y_val > 0] = 1.\n",
    "        #pred_y_val[pred_y_val <=0] = 0.\n",
    "        target_y_val = targets.cpu().data.numpy().flatten()\n",
    "        tu = (v_loss, roc_auc_score(target_y_val,pred_y_val ))\n",
    "        print ('Validation batch: {}, LOSS={}, ROC_AUC={} '.format(i_val, *tu))  \n",
    "\n",
    "        # calculate FPR and TPR, and area-under-curve\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(target_y_val,pred_y_val)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        \n",
    "        # append to lists\n",
    "        all_fpr.append(false_positive_rate)\n",
    "        all_tpr.append(true_positive_rate)\n",
    "        all_thresh.append(thresholds)\n",
    "        all_roc_auc.append(roc_auc)\n",
    "\n",
    "        plt.title('Validation batch: {}, LOSS= {}'.format(i_val, v_loss))\n",
    "        plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.6f' % roc_auc)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlim([-0.1, 1.2])\n",
    "        plt.ylim([-0.1, 1.2])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot all the validation dataset ROC curves\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Family of Validation Dataset ROC curves')\n",
    "for ind in range(len(all_fpr)):\n",
    "    \n",
    "    plt.plot(all_fpr[ind], all_tpr[ind])\n",
    "    #plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([-0.1, 1.2])\n",
    "    plt.ylim([-0.1, 1.2])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "#     plt.yscale('log')\n",
    "#     plt.xscale('log')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_thresh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = 'runs/debug/Pansharpened_model_e{}_b{}_no_aug_GPUPAR.pt'.format(epoch_i, bsize)\n",
    "save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "    }, str(model_path))\n",
    "\n",
    "save(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses epoch losses from the last 66/100 epochs\n",
    "# (forgot to record from epoch 0 since I was going in epoch chunks... it is faster with the GPU!!)\n",
    "plt.plot([l.data.cpu().numpy() for l in losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check one of the test images\n",
    "test_inds = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700]\n",
    "for test_ind in test_inds:\n",
    "    input_img = torch.unsqueeze(variable(dg_dataset_test[test_ind], volatile=True), dim=0) \n",
    "    mask = model(input_img)\n",
    "\n",
    "\n",
    "    mask_im = mask.squeeze().data.cpu().numpy()\n",
    "    mask_binary = mask_im.copy()\n",
    "\n",
    "    fig, ax = plt.subplots(1,3, figsize=(30,10))\n",
    "    thresh = 0\n",
    "    mask_binary[mask_im<=thresh] = 0\n",
    "    mask_binary[mask_im>thresh] = 1\n",
    "    ax[0].imshow(mask_binary)\n",
    "    # ax[0].colorbar()\n",
    "    ax[1].imshow(mask_im)\n",
    "    gt_im = gt_dataset_test[test_ind].numpy().squeeze()\n",
    "    ax[2].imshow(gt_im)\n",
    "    # ax[1].colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    source_im = dg_dataset_test[test_ind].numpy()\n",
    "    plt.imshow(misc.bytescale(np.rollaxis(source_im, 0, 3)))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # temp code to keep running overnight\n",
    "# timeout = time.time() + 60*60*12   # 5 hours from now\n",
    "# while True:\n",
    "#     test = 0\n",
    "#     time.sleep(1)\n",
    "#     if test == 5 or time.time() > timeout:\n",
    "#         break\n",
    "#     test = test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save current state\n",
    "# save = lambda ep: torch.save({\n",
    "#         'model': model.state_dict(),\n",
    "#         'epoch': ep,\n",
    "#         'step': step,\n",
    "#     }, str(model_path))\n",
    "\n",
    "# root = Path('runs/debug')\n",
    "# model_path = root / 'model_e{}_s{}.pt'.format(epoch, step)\n",
    "# ep=epoch\n",
    "# step=i\n",
    "\n",
    "# save(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load model\n",
    "# if model_path.exists():\n",
    "#     state = torch.load(str(model_path))\n",
    "#     epoch = state['epoch']\n",
    "#     step = state['step']\n",
    "#     model.load_state_dict(state['model'])\n",
    "#     print('Restored model, epoch {}, step {:,}'.format(epoch, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (spatial_torch)",
   "language": "python",
   "name": "spatial_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
